

# **![image](https://github.com/CLAREISMO/Machine-Learning-Fundamentals/assets/63759427/4f05d5a8-920b-49f8-afb5-8e0d19da3fff)**<img src="https://media.tenor.com/eT_e-q0D5xoAAAAi/long-livethe-blob-sunglasses.gif" width="50px">


**In this repository, you will find the basics that every enthusiastic programmer needs to enter the amazing world of Machine Learning!**

**I will apply in all the development the most popular automatic differentiation libraries Pytorch and Tensorflow and of course, I will work in the most important programming language in ML Python, which is the base of these amazing libraries.**

**Enjoy!Ôªø**



## ***Data Structures for Algebra***



### ![image](https://github.com/CLAREISMO/Machine-Learning-Fundamentals/assets/63759427/9185fb8a-f966-423e-a849-61e0e4b0940c)

The most common data structure in linear algebra for machine learning is the tensor. Tensors are just a machine learning generalization of vectors or matrices to any number of dimensions.

+ The zero-dimensional tensor is called a scalar-tensor and is a single numerical value. Like the number 25. 

+ A vector tensor is a one-dimensional tensor. It has values in a specific order. It's an ordered list of values. And let's say this vector right here has a length of three. And its elements are X1 X2 X3, all three elements are scalar values. 

+ Going up another dimension, we have matrices. So a tensor matrix has two dimensions. It has a height and a width. So here's a two-by-two tensor matrix with one, two, three four individual elements. We'll talk about the details of numbering and notation later on. 

+ After that, you can continue to generalize to higher dimensional tensors. So this is a tensor of three, for example, a cube. It has one, two, three, four, five, six, seven, and then an eighth element hidden in the back. So it's a tensor of three, two by two by two by two, two wide, two high, two deep. Yeah, so this kind of generalization can go to higher dimensions. You can have four tensors, five tensors, 100 tensors, and 1000 tensors, but it's getting harder and harder for me to draw beyond a 3-tensor.

**We can conclude from the above that:**

+ A zero-dimensional tensor, we can mathematically call it a scalar and represents magnitude only. Just as the number 25 is a single value here. It could mean 25 meters. It represents magnitude only, size only. 

+ A one-dimensional tensor is a vector, and this is typically an array of numbers. And I should also say here that it doesn't have to be typically integers or stream values. It could be strings or character strings, but in machine learning it's typically numerical values, typically float values. So a vector tensor is likely to be a matrix of numbers, a one-dimensional array of numbers. 

+ A two-dimensional tensor is a matrix like a flat table, for example, a square or rectangle, a two-dimensional set of values. 


- And then in three dimensions, we have a three-dimensional tensor which is like this cube drawn here, or like a 3D table. 

+ And then in higher dimensionalities in n dimensions, we call this an n-tensor. So if it's six-dimensional, then it's a 6-tensor. And yes, this is higher dimensional and harder to visualize.


![image](https://github.com/CLAREISMO/Machine-Learning-Fundamentals/assets/63759427/3b5e0137-5bdc-4eb7-9162-e4d52fa74d28)

![image](https://github.com/CLAREISMO/Machine-Learning-Fundamentals/assets/63759427/02a42ff1-90a6-4d6b-ad27-e13ddcd1d62d)



Ôªø

ü§ù








 






